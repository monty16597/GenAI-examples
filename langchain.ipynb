{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2bf73f",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "This repository contains examples of how to use Langchain features. Each example demonstrates a different use case, showcasing the flexibility and power of Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28bc0b",
   "metadata": {},
   "source": [
    "## Create LLM instance\n",
    "First, we need an instance of the LLM. Here, we are using Gemini as a LLM instance and it will be used to process our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee43c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda82053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GOOGLE_API_KEY to use gemini model\n",
    "# https://console.cloud.google.com/apis/credentials",
    "\n",
    "if os.getenv(\"GOOGLE_API_KEY\") is None:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"<YOUR_GOOGLE_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baece6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM instance\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fc592",
   "metadata": {},
   "source": [
    "### Invoke the LLM with single prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b180812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--7b656961-9bdc-4a75-b5b4-f8cff02850c4-0', usage_metadata={'input_tokens': 5, 'output_tokens': 11, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who are you?\")  # Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733db0b",
   "metadata": {},
   "source": [
    "## Invoke the LLM with multiple messages\n",
    "We define a list of messages to send to the LLM. Each message can be of different types, such as user messages or system messages.\n",
    "- HumanMessage: This is a message from the user. Use this to provide input or ask questions as a User.\n",
    "- SystemMessage: This is a message from the system. Use this to set the context or instructions for the LLM.\n",
    "- AIMessage: This is a message from the AI. Use this to provide responses or outputs from the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b21c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Namaste! Welcome to our restaurant! We have a wide variety of delicious North Indian dishes to tantalize your taste buds. Here's a glimpse of what we offer:\\n\\n**Appetizers (Starters):**\\n\\n*   **Paneer Tikka:** Cubes of marinated paneer (Indian cottage cheese) grilled to perfection with onions and bell peppers.\\n*   **Chicken Tikka:** Tender pieces of chicken marinated in yogurt and spices, then grilled.\\n*   **Samosa:** Crispy fried pastries filled with spiced potatoes and peas.\\n*   **Aloo Tikki:** Spiced potato patties, shallow-fried until golden brown.\\n*   **Onion Bhaji:** Finely sliced onions coated in a spiced chickpea flour batter and deep-fried.\\n*   **Vegetable Pakora:** Assorted mixed vegetables coated in a spiced chickpea flour batter and deep-fried.\\n*   **Hara Bhara Kabab:** Patties made with spinach, peas, and potatoes, pan-fried.\\n\\n**Main Courses (Curries & Gravies):**\\n\\n**Vegetarian Delights:**\\n\\n*   **Paneer Butter Masala:** A rich and creamy tomato-based gravy with soft paneer cubes.\\n*   **Palak Paneer:** Fresh spinach cooked with paneer in a flavorful gravy.\\n*   **Dal Makhani:** Black lentils and kidney beans slow-cooked in a creamy, buttery tomato sauce.\\n*   **Chana Masala:** Chickpeas cooked in a spicy and tangy tomato-onion gravy.\\n*   **Aloo Gobi:** Potatoes and cauliflower florets cooked with spices.\\n*   **Mixed Vegetable Curry:** A medley of seasonal vegetables cooked in a fragrant gravy.\\n*   **Malai Kofta:** Deep-fried paneer and potato dumplings served in a rich, creamy cashew-based gravy.\\n*   **Baingan Bharta:** Roasted and mashed eggplant cooked with onions, tomatoes, and spices.\\n\\n**Non-Vegetarian Feasts:**\\n\\n*   **Butter Chicken (Murgh Makhani):** Our signature dish! Tender pieces of tandoori chicken in a rich, creamy tomato gravy.\\n*   **Chicken Tikka Masala:** Grilled chicken tikka simmered in a spiced tomato and onion gravy.\\n*   **Rogan Josh:** Tender lamb or goat meat cooked in a rich, aromatic gravy with Kashmiri chilies and yogurt.\\n*   **Vindaloo (Chicken/Lamb):** A fiery and tangy curry, traditionally made with vinegar and spices.\\n*   **Korma (Chicken/Lamb):** A mild and creamy curry made with yogurt, nuts, and mild spices.\\n*   **Saag Chicken/Lamb:** Chicken or lamb cooked with fresh spinach and spices.\\n\\n**Rice & Biryani:**\\n\\n*   **Basmati Rice:** Fluffy, long-grain steamed rice.\\n*   **Jeera Rice:** Basmati rice flavored with cumin seeds.\\n*   **Vegetable Biryani:** Fragrant basmati rice cooked with mixed vegetables and aromatic spices.\\n*   **Chicken Biryani:** Basmati rice cooked with tender chicken pieces and a blend of spices.\\n*   **Lamb Biryani:** Basmati rice cooked with succulent lamb pieces and aromatic spices.\\n\\n**Breads (Naan & Roti):**\\n\\n*   **Plain Naan:** Soft, leavened flatbread baked in our tandoor.\\n*   **Butter Naan:** Naan brushed with butter.\\n*   **Garlic Naan:** Naan flavored with fresh garlic and cilantro.\\n*   **Peshawari Naan:** Sweet naan stuffed with dried fruits and nuts.\\n*   **Tandoori Roti:** Whole wheat unleavened bread baked in our tandoor.\\n*   **Lachha Paratha:** Layered whole wheat flatbread.\\n\\n**Accompaniments:**\\n\\n*   **Raita:** Yogurt-based side dish with cucumber, mint, or boondi (fried gram flour balls).\\n*   **Pickles:** Assorted Indian pickles.\\n*   **Papad:** Crispy lentil wafers.\\n\\n**Desserts:**\\n\\n*   **Gulab Jamun:** Soft, deep-fried milk-solid dumplings soaked in sugar syrup.\\n*   **Kheer:** Rice pudding cooked with milk, sugar, and cardamom.\\n*   **Ras Malai:** Soft paneer patties soaked in sweetened, thickened milk flavored with cardamom and saffron.\\n\\n**Beverages:**\\n\\n*   **Lassi (Sweet/Salted):** A refreshing yogurt-based drink.\\n*   **Masala Chai:** Spiced Indian tea.\\n*   **Soft Drinks:** Coca-Cola, Sprite, etc.\\n*   **Water:** Still and sparkling.\\n\\nThis is just a selection, and our menu might have more seasonal specials. Please let me know if you have any dietary restrictions or preferences, and I\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MAX_TOKENS', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--c29395ac-6ab3-4492-afc6-1a57c1bae9e4-0', usage_metadata={'input_tokens': 21, 'output_tokens': 1024, 'total_tokens': 1045, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant in a North-Indian Restaurant.\"),\n",
    "    HumanMessage(content=\"What dishes you have in the menu?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559c9c0",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "When you want to use a predefined prompt structure, with placeholders for dynamic content, you can use prompt templates. This allows you to create a consistent format for your prompts while still being able to customize the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e0fc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Welcome! We have a delicious selection of Italian favorites for you today. Our specialties include:\\n\\n*   **Pasta:** We have a variety of classic pasta dishes.\\n*   **Pizza:** Of course, we have pizza! And I must say, we make the **best Margherita pizza in town!**\\n*   **Risotto:** For something creamy and comforting, we also offer risotto.\\n\\nIs there anything specific you're in the mood for, or would you like me to tell you more about any of these categories?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--9a79e6f4-dc1e-41d4-8413-642f7bd73f86-0', usage_metadata={'input_tokens': 38, 'output_tokens': 108, 'total_tokens': 146, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "system_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"cuisine\"],\n",
    "    template=\"You are a helpful assistant in a {cuisine} restaurant.\",\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt_template.format(cuisine=\"Italian\")),\n",
    "    SystemMessage(content=\"You have pasta, pizza, and risotto. We make best margerita pizza in town!\"),\n",
    "    HumanMessage(content=\"What dishes you have in the menu?\"),\n",
    "]\n",
    "\n",
    "llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0822e60",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Chains are useful when you want to combine two or more actions which create a workflow. Chains allow user to link multiple components together, passing the output of one component as the input to another. This is useful for building more sophisticated applications that require multiple steps or components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448c3d8",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain\n",
    "SimpleSequentialChain is a type of chain that allows you to execute multiple chains in sequence. The output of one chain becomes the input for the next chain in the sequence. Key feature is, it only accepts a single input variable for all the chains in the sequence. This is useful when you want to create a workflow where the output of one step is directly used as the input for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f99b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary components for prompt templates and chains\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99f7a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/bybb603x54vf1kyhghxkssbc0000gn/T/ipykernel_48054/38766143.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  type_of_rest = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "# type_of_rest: This instance will be used to generate a menu and specialty dish based on the cuisine type.\n",
    "type_of_rest = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"cuisine_type\"],\n",
    "        template=\"You are a helpful assistant in a {cuisine_type} restaurant. When asked about the menu, list 5 names of the dishes and 1 specialty dish with ingredients.\",\n",
    "    ),\n",
    "    output_key=\"ingredients\",\n",
    ")\n",
    "\n",
    "# list_alergens: This instance will be used to analyze the specialty dish for allergens.\n",
    "list_alergens = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"dish\",],\n",
    "        template=\"You are a food quality analyst. You test food allergens and quality when asked about the speciality dish. Provide list of names of allergens into given dish. No any additional information, just list the allergens. \\n Dish Details: {dish}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08dbe4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Italian Cuisine', 'output': 'Gluten\\nEggs\\nMilk\\nNuts\\nSoy'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple chain and inject input for the type of restaurant. This will invoke the first chain, which will return the menu and specialty dish. The output of this chain will be passed to the second chain, which will analyze the specialty dish for allergens.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "chains = SimpleSequentialChain(chains=[type_of_rest, list_alergens]) # Order of the chain matters\n",
    "chains.invoke(\"Italian Cuisine\") # only one input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2b2e6",
   "metadata": {},
   "source": [
    "### SequentialChain\n",
    "SequentialChain is a more flexible type of chain that allows you to define multiple input variables for each chain in the sequence. This means you can pass different inputs to each chain, allowing for more complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0334dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "# type_of_rest: This instance will be used to generate a menu and specialty dish based on the cuisine type.\n",
    "type_of_rest = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"cuisine_type\", \"dish_name\"],\n",
    "        template=\"You are a helpful assistant in a {cuisine_type} restaurant. When asked about the menu, list ingredients of the the {dish_name}\",\n",
    "    ),\n",
    "    output_key=\"ingredients\", # This will be used as input for the next chain (order_chain\n",
    ")\n",
    "\n",
    "# order_chain: This instance will be used to analyze the specialty dish for allergens.\n",
    "order_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"dish_name\", \"ingredients\"],\n",
    "        template=\"You are a food quality analyst. You test food allergens and quality when asked about the speciality dish. Provide list of names of allergens into given dish. No any additional information, just list the allergens. \\n Dish Name: {dish_name}, Ingredients: {ingredients}\",\n",
    "    ),\n",
    "    output_key=\"allergens\", # This will be used as output of the chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1699d231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine_type': 'Italian Cuisine',\n",
       " 'dish_name': 'Margerita Pizza',\n",
       " 'allergens': 'Milk\\nWheat'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chains = SequentialChain(\n",
    "    chains=[type_of_rest, order_chain],\n",
    "    input_variables=[\"cuisine_type\", \"dish_name\"],\n",
    "    output_variables=[\"dish_name\", \"allergens\"],\n",
    ")\n",
    "\n",
    "chains.invoke({\"cuisine_type\": \"Italian Cuisine\", \"dish_name\": \"Margerita Pizza\"})  # Example usage with multiple inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda065a0",
   "metadata": {},
   "source": [
    "Different type of chains are available in Langchain, checkout the [this link](https://python.langchain.com/docs/modules/chains/overview) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613d79e",
   "metadata": {},
   "source": [
    "## Memory\n",
    "Memory is a feature that allows you to store and retrieve information across multiple interactions with the LLM. It stores convertaion history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ff0871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/bybb603x54vf1kyhghxkssbc0000gn/T/ipykernel_48054/1532302122.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/vd/bybb603x54vf1kyhghxkssbc0000gn/T/ipykernel_48054/1532302122.py:5: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 1: Ciao! Welcome to our trattoria! I'm so excited to tell you about our delicious offerings. We have a wonderful selection of authentic Italian dishes that I'm sure you'll love.\n",
      "\n",
      "Let's see, from our antipasti, we have the classic **Bruschetta al Pomodoro**, which is toasted bread rubbed with garlic and topped with fresh tomatoes, basil, and olive oil. Then, for our primi piatti, we have a delightful **Spaghetti Carbonara**, made with eggs, hard cheese, cured pork, and black pepper. We also offer a rich and flavorful **Lasagne alla Bolognese**, with layers of pasta, béchamel sauce, and a hearty meat ragù.\n",
      "\n",
      "Moving on to our secondi piatti, we have a tender **Pollo alla Cacciatora**, which is chicken braised with tomatoes, onions, herbs, and often wine. And for a taste of the sea, we have our exquisite **Gamberi all'Aglio**, which are succulent shrimp sautéed with garlic, white wine, and parsley.\n",
      "\n",
      "So, to answer your question directly, the five dishes I've just mentioned are:\n",
      "\n",
      "*   Bruschetta al Pomodoro\n",
      "*   Spaghetti Carbonara\n",
      "*   Lasagne alla Bolognese\n",
      "*   Pollo alla Cacciatora\n",
      "*   Gamberi all'Aglio\n",
      "query 2: We serve Italian cuisine.\n",
      "query 3: 15\n",
      "query 4: 16\n",
      "query 5: We serve Italian cuisine.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "print(\"query 1:\", conversation.invoke(\"You are a assistant in a Italian Restaurant. What dishes you have in the menu? just list the 5 names of the dishes without description.\").get(\"response\"))\n",
    "print(\"query 2:\", conversation.invoke(\"which type of cuisine do you serve? just give me the answer. No additional information.\").get(\"response\"))  # This will give Italian as an answer\n",
    "print(\"query 3:\", conversation.invoke(\"What 5 + 10? Just give me the answer. No additional information.\").get(\"response\"))\n",
    "print(\"query 4:\", conversation.invoke(\"What 11 + 5? Just give me the answer. No additional information.\").get(\"response\")) # Here, first prompt will be forgotten as we are keeping only last 1 message in the memory\n",
    "print(\"query 5:\", conversation.invoke(\"which type of cuisine do you serve? just give me the answer. No additional information.\").get(\"response\"))  # This will give Italian as an answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24674351",
   "metadata": {},
   "source": [
    "### Limited Memory\n",
    "If you want to store limited number of messages in the memory, you can use `ConversationBufferWindowMemory`. This allows you to keep a fixed number of messages in the memory, which can be useful for maintaining context without overwhelming the LLM with too much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8a1cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/bybb603x54vf1kyhghxkssbc0000gn/T/ipykernel_48054/3667280143.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)  # Keep last 1 message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query 1: Ciao! Welcome to our little slice of Italy! I'd be absolutely delighted to tell you about some of our most popular dishes. We have a wonderful selection, but if you're looking for just five names, here are some of our absolute favorites:\n",
      "\n",
      "1.  Spaghetti Carbonara\n",
      "2.  Margherita Pizza\n",
      "3.  Lasagna Bolognese\n",
      "4.  Chicken Parmesan\n",
      "5.  Tiramisu\n",
      "query 2: Italian\n",
      "query 3: 15\n",
      "query 4: 16\n",
      "query 5: I do not serve any type of cuisine. I am a large language model, trained by Google.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)  # Keep last 1 message\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "print(\"query 1:\", conversation.invoke(\"You are a assistant in a Italian Restaurant. What dishes you have in the menu? just list the 5 names of the dishes without description.\").get(\"response\"))\n",
    "print(\"query 2:\", conversation.invoke(\"which type of cuisine do you serve? just give me the answer. No additional information.\").get(\"response\"))  # This will give Italian as an answer\n",
    "print(\"query 3:\", conversation.invoke(\"What 5 + 10? Just give me the answer. No additional information.\").get(\"response\"))\n",
    "print(\"query 4:\", conversation.invoke(\"What 11 + 5? Just give me the answer. No additional information.\").get(\"response\")) # Here, first prompt will be forgotten as we are keeping only last 1 message in the memory\n",
    "print(\"query 5:\", conversation.invoke(\"which type of cuisine do you serve? just give me the answer. No additional information.\").get(\"response\"))  # This won't return Italian as an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2074457",
   "metadata": {},
   "source": [
    "## Tools\n",
    "Tools perform specifc tasks based on the input provided. Tools can be predefined or custom-built. i.e. We can use Google Search tool to fetch real-time information from the web or we can create custom tools to perform specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2df7e0",
   "metadata": {},
   "source": [
    "### Predefined Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ff8d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skyli997/Desktop/langchain/.venv/lib/python3.13/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: duckduckgo_search, \n",
      "\n",
      "Result: Jun 17, 2025 · So the question now is: Is LangChain still needed in 2025? In this article, we’ll explore that question by comparing … Jul 8, 2025 · LangChain, an AI infrastructure startup providing tools to build and monitor LLM-powered applications, is raising a new round of … Jun 17, 2025 · Cybersecurity researchers have disclosed a now-patched security flaw in LangChain's LangSmith platform that could be … Mar 5, 2025 · LangChain launched its first course, 'Introduction to LangGraph,' through LangChain Academy, focusing on building agentic and … Jul 9, 2025 · Cofounders Harrison Chase and Ankush Gola started LangChain in 2023 as an open source software that helped engineers …\n"
     ]
    }
   ],
   "source": [
    "# Import relevant functionality\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun() # Create a search tool using DuckDuckGo\n",
    "tools = [search] # List of tools to be used by the agent. Used in llm to bind tools\n",
    "tool_map = {tool.name: tool for tool in tools} # Useful to easily invoke the tool by name\n",
    "agent = llm.bind_tools(tools) # Create an agent with the tools\n",
    "resp = agent.invoke(\"What is the latest news about LangChain? search into Duckduckgo\") # Example usage with the search tool\n",
    "\n",
    "# Iterate through the tool calls and run the corresponding tool\n",
    "for call in resp.tool_calls:\n",
    "    if call.get(\"name\") in tool_map:\n",
    "        tool = tool_map[call.get(\"name\")]\n",
    "        result = tool.run(call.get(\"args\"))\n",
    "        print(f\"Tool: {tool.name}, \\n\\nResult: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf5065",
   "metadata": {},
   "source": [
    "### Custom Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477733a",
   "metadata": {},
   "source": [
    "#### Example of direct tool invocation\n",
    "Here. we invoke tools direclty without using LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cdeeb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add: 5.0\n",
      "Multiply: 6.0\n",
      "Divide by zero: Division by zero is not allowed.\n",
      "Divide: 0.6666666666666666\n",
      "Subtract: -1.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    This is a custom tool that takes two floats and returns their sum.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    This is a custom tool that takes two floats and returns their product.\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float | str:\n",
    "    \"\"\"\n",
    "    This is a custom tool that takes two floats and returns their division.\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        return \"Division by zero is not allowed.\"\n",
    "    return a / b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    This is a custom tool that takes two floats and returns their difference.\n",
    "    \"\"\"\n",
    "    return a - b\n",
    "\n",
    "# Direct use of tools\n",
    "print(\"Add:\", add.invoke({\"a\": 2, \"b\": 3}))\n",
    "print(\"Multiply:\", multiply.invoke({\"a\": 2, \"b\": 3}))\n",
    "print(\"Divide by zero:\", divide.invoke({\"a\": 2, \"b\": 0}))  # This will return \"Division by zero is not allowed.\"\n",
    "print(\"Divide:\", divide.invoke({\"a\": 2, \"b\": 3}))\n",
    "print(\"Subtract:\", subtract.invoke({\"a\": 2, \"b\": 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc0a69",
   "metadata": {},
   "source": [
    "#### Example of binding custom tools to LLM\n",
    "In this example, LLM will return the tool name and input parameters to invoke the tool and we add code to invoke the tool based on the output from LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e224577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: add, Input: {'a': 2.0, 'b': 3.0}, Result: 5.0\n"
     ]
    }
   ],
   "source": [
    "# Bind tools to the LLM and invoke them\n",
    "tools = [add, multiply, divide, subtract]\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "agent = llm.bind_tools(tools)\n",
    "response = agent.invoke(\"What is 2 + 3?\")\n",
    "\n",
    "for call in response.tool_calls:\n",
    "    if call.get(\"name\") in tool_map:\n",
    "        tool = tool_map[call.get(\"name\")]\n",
    "        result = tool.run(call.get(\"args\"))\n",
    "        print(f\"Tool: {tool.name}, Input: {call.get('args')}, Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf025bd",
   "metadata": {},
   "source": [
    "## Agents\n",
    "Agents are llm with tools which can invoke tools dynamically based on the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c5711cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vd/bybb603x54vf1kyhghxkssbc0000gn/T/ipykernel_48054/2308310953.py:32: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "/var/folders/vd/bybb603x54vf1kyhghxkssbc0000gn/T/ipykernel_48054/2308310953.py:40: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"What is (5 + 3) * 2 divided by 4?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to perform the following calculation: (5 + 3) * 2 / 4.\n",
      "First, I will calculate the sum inside the parentheses.\n",
      "Then, I will multiply the result by 2.\n",
      "Finally, I will divide the product by 4.\n",
      "I will use the `add` tool for the first step.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"add\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 5,\n",
      "    \"b\": 3\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m8.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe result of 5 + 3 is 8. Now I need to multiply this result by 2. I will use the `multiply` tool for this step.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"multiply\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 8.0,\n",
      "    \"b\": 2\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m16.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe result of (5 + 3) * 2 is 16. Now I need to divide this result by 4. I will use the `divide` tool for this step.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"divide\",\n",
      "  \"action_input\": {\n",
      "    \"a\": 16.0,\n",
      "    \"b\": 4\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe result of the calculation (5 + 3) * 2 / 4 is 4.0. I have completed all the steps.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"4.0\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import tool\n",
    "\n",
    "# 1. Define calculator functions using @tool decorator\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtracts b from a.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divides a by b. Raises error if dividing by zero.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero.\")\n",
    "    return a / b\n",
    "\n",
    "# 2. Create list of tools\n",
    "tools = [add, subtract, multiply, divide]\n",
    "\n",
    "# 4. Initialize the agent with tools and LLM\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, # Use structured chat agent type when tools have multiple inputs\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. Run the agent with a query\n",
    "agent.run(\"What is (5 + 3) * 2 divided by 4?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
